---
title: "607 - Project 3 - Soft Skills"
author: "Jose Mawyin"
date: "10/10/2019"
output: pdf_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
library(rvest)
library(dplyr)
library(tm)
library(SnowballC)
library(wordcloud)
library(RCurl)
library(XML)
library(RColorBrewer)
library(udpipe)
library(textrank)
```

## Scraping Indeed for Soft Skills Relevant to a Data Scientist


**First, lets load a serch page from Indeed with the query of Data Scientist Jobs based in a 15 mile radious around New York.**
```{r}

url <- "https://www.indeed.com/jobs?q=data+scientist&l=new+york&radius=15"
page <- xml2::read_html(url)
```




**The code below scraps through indeed and sorts the info from job postings into 4 different columns: job_title, company_name, job_location, job_description**

The code is not working properly as it only loads  19 entries. I think that at that point it finds a not working page, gets stuck and times out.

```{r}

page_result_start <- 10 # starting page 
page_result_end <- 800 # last page results
page_results <- seq(from = page_result_start, to = page_result_end, by = 10)
 
full_df <- data.frame()
for(i in seq_along(page_results)) {
  
  first_page_url <- "https://www.indeed.com/jobs?q=data+scientist&l=new+york&radius=15"
  url <- paste0(first_page_url, "&start=", page_results[i])
  page <- xml2::read_html(url)
  # Sys.sleep pauses R for two seconds before it resumes
  # Putting it there avoids error messages such as "Error in open.connection(con, "rb") : Timeout was reached"
  Sys.sleep(2)
  
  #get the job title
  job_title <- page %>% 
    rvest::html_nodes("div") %>%
    rvest::html_nodes(xpath = '//a[@data-tn-element = "jobTitle"]') %>%
    rvest::html_attr("title")
  
  #get the company name
  company_name <- page %>% 
  rvest::html_nodes(".company") %>%
  rvest::html_text() %>%
  stringi::stri_trim_both()
  
  
  #get job location
  job_location <- page %>% 
  rvest::html_nodes(".location") %>%
  rvest::html_text()
  
  # get links
  links <- page %>% 
  rvest::html_nodes("div") %>%
  rvest::html_nodes(xpath = '//*[@data-tn-element="jobTitle"]') %>%
  rvest::html_attr("href")
  
  job_description <- c()
  for(i in seq_along(links)) {
    
    url <- paste0("https://www.indeed.com/", links[i])
    page <- xml2::read_html(url)
    
    job_description[[i]] <- page %>%
  rvest::html_nodes("span")  %>% 
  rvest::html_nodes(xpath = '//*[@class="jobsearch-JobComponent-description  icl-u-xs-mt--md  "]') %>% 
  rvest::html_text() %>%
  stringi::stri_trim_both()
  }
  df <- data.frame(job_title, company_name, job_location, job_description)
  full_df <- rbind(full_df, df)
}
```

```{r}
dim(full_df)
str(full_df)
colnames(full_df)
```

###Saving data.frame containing Job Descriptions into a .CSV file
```{r}
write.csv(full_df, "/Users/josemawyin/Library/Mobile Documents/com~apple~CloudDocs/Data Science Masters /607/607Project3/IndeedScrap.csv")
```



```{r}
library(tm)
library(SnowballC)
library(wordcloud)
library(RCurl)
library(XML)
library(RColorBrewer)
library(udpipe)
library(textrank)
```



###**Using UDPIPE Package with Indeed Data Scientist Data**
```{r}
comments <- subset(full_df)
ud_model <- udpipe_download_model(language = "english-ewt")
ud_model <- udpipe_load_model(ud_model$file_model)
x <- udpipe_annotate(ud_model, x = comments$job_description)
x <- as.data.frame(x)
colnames(x)
```

```{r}
str(x)
```


###**Textrank (word network ordered by Google Pagerank)**
"Textrank is an algorithm implemented in the textrank R package. The algorithm allows to summarise text and as well allows to extract keywords. This is done by constructing a word network by looking if words are following one another. On top of that network the ‘Google Pagerank’ algorithm is applied to extract relevant words after which relevant words which are following one another are combined to get keywords. "
```{r}
stats2 <- textrank_keywords(x$lemma, 
                          relevant = x$upos %in% c("NOUN", "ADJ"), 
                          ngram_max = 8, sep = " ")
stats2 <- subset(stats2$keywords, ngram > 1 & freq >= 20)
dim(stats2)
top20.Page.Rank <- stats2[1:20,]
View(top20.Page.Rank)
```

###** Extracting Keywords using Rapid Automatic Keyword Extraction (RAKE)**
RAKE which is an acronym for Rapid Automatic Keyword Extraction. It looks for keywords by looking to a contiguous sequence of words which do not contain irrelevant words. Namely by:

1. calculating a score for each word which is part of any candidate keyword, this is done by among the words of the candidate keywords, the algorithm looks how many times each word is occurring and how many times it co-occurs with other words each word gets a score which is the ratio of the word degree (how many times it co-occurs with other words) to the word frequency
2. a RAKE score for the full candidate keyword is calculated by summing up the scores of each of the words which define the candidate keyword.
```{r}
stats3 <- keywords_rake(x = x, 
                      term = "token", group = c("doc_id", "paragraph_id", "sentence_id"),
                      #relevant = x$upos %in% c("NOUN", "ADJ"),
                      relevant = x$upos %in% c("NOUN", "ADJ"),
                      ngram_max = 4)

top20.Rake <- head(subset(stats3, freq > 40),20)
View(top20.Rake)
```



###**Phrases**
Phrases  are defined as a sequence of Parts of Speech Tags. Common type of phrases are noun phrases or verb phrases. How does this work? Parts of Speech tags are recoded to one of the following one-letters: (A: adjective, C: coordinating conjuction, D: determiner, M: modifier of verb, N: noun or proper noun, P: preposition). Next you can define a regular expression to indicate a sequence of parts of speech tags which you want to extract from the text.
```{r}
## Simple noun phrases (a adjective+noun, pre/postposition, optional determiner and another adjective+noun)
x$phrase_tag <- as_phrasemachine(x$upos, type = "upos")
stats <- keywords_phrases(x = x$phrase_tag, term = x$token, 
                         pattern = "(A|N)+N(P+D*(A|N)*N)*", 
                         is_regex = TRUE, ngram_max = 3, detailed = TRUE)
top20.Phrases <- head(subset(stats, ngram > 2),20)
View(top20.Phrases)
```

###**Using dependency parsing output to get the nominal subject and the adjective of it**
Dependency Parsing: When you executed the annotation using udpipe, the dep_rel field indicates how words are related to one another. A token is related to the parent using token_id and head_token_id. The dep_rel field indicates how words are linked to one another. The type of relations are defined at http://universaldependencies.org/u/dep/index.html. For this exercise we are going to take the words which have as dependency relation nsubj indicating the nominal subject and we are adding to that the adjective which is changing the nominal subject.

In this way we can combine what are people talking about with the adjective they use when they talk about the subject.
```{r}
stats <- merge(x, x, 
           by.x = c("doc_id", "paragraph_id", "sentence_id", "head_token_id"),
           by.y = c("doc_id", "paragraph_id", "sentence_id", "token_id"),
           all.x = TRUE, all.y = FALSE, 
           suffixes = c("", "_parent"), sort = FALSE)
stats <- subset(stats, dep_rel %in% "nsubj" & upos %in% c("NOUN") & upos_parent %in% c("ADJ"))
stats$term <- paste(stats$lemma_parent, stats$lemma, sep = " ")
stats <- txt_freq(stats$term)
head(stats,20)
```

###**Extracting only nouns**
An easy way in order to find keywords is by looking at nouns. As each term has a Parts of Speech tag if you annotated text using the udpipe package, you can easily do this as follows.
```{r}


stats <- subset(x, upos %in% "NOUN")
stats <- txt_freq(x = stats$lemma)
library(lattice)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 30), col = "cadetblue", main = "Most occurring nouns", xlab = "Freq")
```


###Finding Matches of "Soft Skills" list appearing in Job Descriptions**
```{r}
Soft.Skills <- read.csv(file="/Users/josemawyin/Library/Mobile Documents/com~apple~CloudDocs/Data Science Masters /607/607Project3/generic_skill_list.csv", header=TRUE, sep=",", stringsAsFactors=FALSE) 


```

**Soft Skills and Job Descriptions to Lower Case
```{r}
require(tidyverse)

s.skills <- Soft.Skills %>% mutate(x = tolower(x))
j.descriptions <- full_df %>% mutate(job_description = tolower(job_description))

head(s.skills)
View(s.skills)
head(j.descriptions)
```


```{r}
library(stringi)
skill.count <- data.frame(matrix(NA, nrow = row.count, ncol = 2))
SL.size <- nrow(s.skills)

for (i in 1:SL.size) {
  string <- s.skills[i,2] %>% as.String()
  string.count <- stri_count_regex(j.descriptions$job_description, string) %>% as.data.frame() %>% colSums()
  skill.count[i,1] <- s.skills[i,2]
  skill.count[i,2] <- string.count
}  

```

```{r}
top.soft.skills <- arrange(skill.count, desc(X2))
colnames(top.soft.skills) <- c("Soft Skill", "Frequency in Job Description")
top.soft.skills
```

###Saving data.frame containing frequency of top.soft.skills into a .CSV file
```{r}
write.csv(top.soft.skills, "/Users/josemawyin/Library/Mobile Documents/com~apple~CloudDocs/Data Science Masters /607/607Project3/TopSoftSkills.csv")
```







